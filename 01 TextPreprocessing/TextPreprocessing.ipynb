{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BHariKrishnaReddy/NLP/blob/main/01%20TextPreprocessing/TextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://www.kaggle.com/code/harikrishnareddyb/textpreprocessing\" target=\"_blank\"><img alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a><br>\n"
      ],
      "metadata": {
        "id": "_R18bqZvPgXz"
      },
      "id": "_R18bqZvPgXz"
    },
    {
      "cell_type": "markdown",
      "id": "9b7d6863",
      "metadata": {
        "id": "9b7d6863"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is the process of breaking down text into smaller units, called tokens. Tokens are typically words, phrases, or sentences that can be further processed for various natural language processing (NLP) tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8a82eb",
      "metadata": {
        "id": "cc8a82eb"
      },
      "outputs": [],
      "source": [
        "para =\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of 'de Finibus Bonorum et Malorum' (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, 'Lorem ipsum dolor sit amet..', comes from a line in section 1.10.32.The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from 'de Finibus Bonorum et Malorum' by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd49f21",
      "metadata": {
        "id": "5bd49f21"
      },
      "source": [
        "#### Tokenization using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9dbb04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d9dbb04",
        "outputId": "ab9e94de-cff5-48a3-ca48-840ce3396178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum.Contrary', 'to', 'popular', 'belief', ',', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', '.', 'It', 'has', 'roots', 'in', 'a', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', ',', 'making', 'it', 'over', '2000', 'years', 'old', '.', 'Richard', 'McClintock', ',', 'a', 'Latin', 'professor', 'at', 'Hampden-Sydney', 'College', 'in', 'Virginia', ',', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', ',', 'consectetur', ',', 'from', 'a', 'Lorem', 'Ipsum', 'passage', ',', 'and', 'going', 'through', 'the', 'cites', 'of', 'the', 'word', 'in', 'classical', 'literature', ',', 'discovered', 'the', 'undoubtable', 'source', '.', 'Lorem', 'Ipsum', 'comes', 'from', 'sections', '1.10.32', 'and', '1.10.33', 'of', \"'de\", 'Finibus', 'Bonorum', 'et', 'Malorum', \"'\", '(', 'The', 'Extremes', 'of', 'Good', 'and', 'Evil', ')', 'by', 'Cicero', ',', 'written', 'in', '45', 'BC', '.', 'This', 'book', 'is', 'a', 'treatise', 'on', 'the', 'theory', 'of', 'ethics', ',', 'very', 'popular', 'during', 'the', 'Renaissance', '.', 'The', 'first', 'line', 'of', 'Lorem', 'Ipsum', ',', \"'Lorem\", 'ipsum', 'dolor', 'sit', 'amet', '..', \"'\", ',', 'comes', 'from', 'a', 'line', 'in', 'section', '1.10.32.The', 'standard', 'chunk', 'of', 'Lorem', 'Ipsum', 'used', 'since', 'the', '1500s', 'is', 'reproduced', 'below', 'for', 'those', 'interested', '.', 'Sections', '1.10.32', 'and', '1.10.33', 'from', \"'de\", 'Finibus', 'Bonorum', 'et', 'Malorum', \"'\", 'by', 'Cicero', 'are', 'also', 'reproduced', 'in', 'their', 'exact', 'original', 'form', ',', 'accompanied', 'by', 'English', 'versions', 'from', 'the', '1914', 'translation', 'by', 'H.', 'Rackham', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Tokenize text into words\n",
        "words = word_tokenize(para)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f14447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16f14447",
        "outputId": "0f3c7411-ecee-4dba-a9a4-8f095092a1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.', \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\", 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.', 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Contrary to popular belief, Lorem Ipsum is not simply random text.', 'It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old.', 'Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.', \"Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of 'de Finibus Bonorum et Malorum' (The Extremes of Good and Evil) by Cicero, written in 45 BC.\", 'This book is a treatise on the theory of ethics, very popular during the Renaissance.', \"The first line of Lorem Ipsum, 'Lorem ipsum dolor sit amet..', comes from a line in section 1.10.32.The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested.\", \"Sections 1.10.32 and 1.10.33 from 'de Finibus Bonorum et Malorum' by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\"]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize text into sentences\n",
        "sentences = sent_tokenize(para)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac2407c4",
      "metadata": {
        "id": "ac2407c4"
      },
      "source": [
        "#### Tokenization using SPACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0acb9e76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acb9e76",
        "outputId": "528a64d0-814e-4f75-ad68-e784c0d73b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.', 'Contrary', 'to', 'popular', 'belief', ',', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', '.', 'It', 'has', 'roots', 'in', 'a', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', ',', 'making', 'it', 'over', '2000', 'years', 'old', '.', 'Richard', 'McClintock', ',', 'a', 'Latin', 'professor', 'at', 'Hampden', '-', 'Sydney', 'College', 'in', 'Virginia', ',', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', ',', 'consectetur', ',', 'from', 'a', 'Lorem', 'Ipsum', 'passage', ',', 'and', 'going', 'through', 'the', 'cites', 'of', 'the', 'word', 'in', 'classical', 'literature', ',', 'discovered', 'the', 'undoubtable', 'source', '.', 'Lorem', 'Ipsum', 'comes', 'from', 'sections', '1.10.32', 'and', '1.10.33', 'of', \"'\", 'de', 'Finibus', 'Bonorum', 'et', 'Malorum', \"'\", '(', 'The', 'Extremes', 'of', 'Good', 'and', 'Evil', ')', 'by', 'Cicero', ',', 'written', 'in', '45', 'BC', '.', 'This', 'book', 'is', 'a', 'treatise', 'on', 'the', 'theory', 'of', 'ethics', ',', 'very', 'popular', 'during', 'the', 'Renaissance', '.', 'The', 'first', 'line', 'of', 'Lorem', 'Ipsum', ',', \"'\", 'Lorem', 'ipsum', 'dolor', 'sit', 'amet', '..', \"'\", ',', 'comes', 'from', 'a', 'line', 'in', 'section', '1.10.32.The', 'standard', 'chunk', 'of', 'Lorem', 'Ipsum', 'used', 'since', 'the', '1500s', 'is', 'reproduced', 'below', 'for', 'those', 'interested', '.', 'Sections', '1.10.32', 'and', '1.10.33', 'from', \"'\", 'de', 'Finibus', 'Bonorum', 'et', 'Malorum', \"'\", 'by', 'Cicero', 'are', 'also', 'reproduced', 'in', 'their', 'exact', 'original', 'form', ',', 'accompanied', 'by', 'English', 'versions', 'from', 'the', '1914', 'translation', 'by', 'H.', 'Rackham', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\") # a small English language model that has been trained on a large corpus of text data\n",
        "\n",
        "# Tokenize text\n",
        "doc = nlp(para)\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea19b46",
      "metadata": {
        "id": "7ea19b46"
      },
      "source": [
        "#### Tokenization using Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0eda77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0eda77",
        "outputId": "f7a3f28e-9d03-4d34-cfc4-21472598919b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', 'when', 'an', 'unknown', 'printer', 'took', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'type', 'specimen', 'book', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', 'remaining', 'essentially', 'unchanged', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', 'Contrary', 'to', 'popular', 'belief', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', 'It', 'has', 'roots', 'in', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', 'making', 'it', 'over', '2000', 'years', 'old', 'Richard', 'McClintock', 'Latin', 'professor', 'at', 'Hampden', 'Sydney', 'College', 'in', 'Virginia', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', 'consectetur', 'from', 'Lorem', 'Ipsum', 'passage', 'and', 'going', 'through', 'the', 'cites', 'of', 'the', 'word', 'in', 'classical', 'literature', 'discovered', 'the', 'undoubtable', 'source', 'Lorem', 'Ipsum', 'comes', 'from', 'sections', '10', '32', 'and', '10', '33', 'of', 'de', 'Finibus', 'Bonorum', 'et', 'Malorum', 'The', 'Extremes', 'of', 'Good', 'and', 'Evil', 'by', 'Cicero', 'written', 'in', '45', 'BC', 'This', 'book', 'is', 'treatise', 'on', 'the', 'theory', 'of', 'ethics', 'very', 'popular', 'during', 'the', 'Renaissance', 'The', 'first', 'line', 'of', 'Lorem', 'Ipsum', 'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'comes', 'from', 'line', 'in', 'section', '10', '32', 'The', 'standard', 'chunk', 'of', 'Lorem', 'Ipsum', 'used', 'since', 'the', '1500s', 'is', 'reproduced', 'below', 'for', 'those', 'interested', 'Sections', '10', '32', 'and', '10', '33', 'from', 'de', 'Finibus', 'Bonorum', 'et', 'Malorum', 'by', 'Cicero', 'are', 'also', 'reproduced', 'in', 'their', 'exact', 'original', 'form', 'accompanied', 'by', 'English', 'versions', 'from', 'the', '1914', 'translation', 'by', 'Rackham']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Tokenize text\n",
        "sktokens = vectorizer.build_tokenizer()(para)\n",
        "print(sktokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "310e0e19",
      "metadata": {
        "id": "310e0e19"
      },
      "source": [
        "#### Tokenization using KERAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb7b53f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb7b53f",
        "outputId": "7375c7cc-41fa-45b3-fe74-21a2c4ca9549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 8, 8, 8, 8, 20, 20, 8, 25, 8, 20, 20, 20, 14]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer # from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Create a tokenizer instance\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the text data\n",
        "tokenizer.fit_on_texts(para) # builds the vocabulary of tokens based on the frequency of words in the text\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.texts_to_sequences([para])[0]  # the text into a list of sequences of integers, where each integer represents the index of a word in the vocabulary\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming is a process of reducing words to their root or base form, which is called a \"stem\"."
      ],
      "metadata": {
        "id": "gOo5bmtmTnlF"
      },
      "id": "gOo5bmtmTnlF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Porter Stemmer: \n",
        "  * one of the oldest and most widely used. \n",
        "  * Uses a set of rules to remove common suffixes, in order to obtain  root form. \n",
        "  * Simple and fast, but it may not always produce the most accurate results.\n",
        "\n",
        "2. Snowball Stemmer(Porter2): \n",
        "  * Improved version of the original Porter Stemmer(accurate and efficient) especially for English words.\n",
        "  * This Stemmer supports multiple languages.\n",
        "3. Lancaster Stemmer: \n",
        "  * more aggressive than the Porter Stemmer.\n",
        "  * It often produces results with excessive stemming(meaningless).\n",
        "\n",
        "4. Regex-based Stemmers: \n",
        "  * flexible and customizable, as you can define your own rules for stemming based on specific requirements.\n",
        "\n",
        " `commonly used one is spacy Snowball Stemmer(Porter2) for Stemming.`\n",
        "\n",
        "> Lemmatization is preffered over stemming in most of the cases."
      ],
      "metadata": {
        "id": "PorIIjS6o7Dl"
      },
      "id": "PorIIjS6o7Dl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming using NLTK"
      ],
      "metadata": {
        "id": "dE4u5z8haARL"
      },
      "id": "dE4u5z8haARL"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "porter,lancaster,snowball = PorterStemmer(), LancasterStemmer() , SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "word_list = ['was','mice','mouse',\"connections\", \"connecting\", \"connects\", \"connected\",'eating','eats','eaten','puts','putting','generous','generate','generously','generation']\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEv5HN9o6C5N",
        "outputId": "096fed1f-4cc5-470c-825e-d3671f668f8f"
      },
      "id": "tEv5HN9o6C5N",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n",
            "was                 wa                  was                 was                           was                                     \n",
            "mice                mice                mice                mic                           mic                                     \n",
            "mouse               mous                mous                mous                          mous                                    \n",
            "connections         connect             connect             connect                       connection                              \n",
            "connecting          connect             connect             connect                       connect                                 \n",
            "connects            connect             connect             connect                       connect                                 \n",
            "connected           connect             connect             connect                       connected                               \n",
            "eating              eat                 eat                 eat                           eat                                     \n",
            "eats                eat                 eat                 eat                           eat                                     \n",
            "eaten               eaten               eaten               eat                           eaten                                   \n",
            "puts                put                 put                 put                           put                                     \n",
            "putting             put                 put                 put                           putt                                    \n",
            "generous            gener               generous            gen                           generou                                 \n",
            "generate            gener               generat             gen                           generat                                 \n",
            "generously          gener               generous            gen                           generously                              \n",
            "generation          gener               generat             gen                           generation                              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemmatization looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words. The lemma of ‘was’ is ‘be’ and the lemma of ‘mice’ is ‘mouse’.\n"
      ],
      "metadata": {
        "id": "Spi60TJuZAS4"
      },
      "id": "Spi60TJuZAS4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization with NLTK"
      ],
      "metadata": {
        "id": "QPBYuCPIhiwf"
      },
      "id": "QPBYuCPIhiwf"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create a WordNetLemmatizer instance\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Text for lemmatization\n",
        "sentence = \"was mice mouse put putting eats taken took connects connecting, I saw eighteen mice today and todays and yesterday\"\n",
        "\n",
        "# Lemmatize the text\n",
        "tokens = sentence.split()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "id": "6Ft0IclyZErj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cfc293-eea5-41d5-aa23-b973a5dc540b"
      },
      "id": "6Ft0IclyZErj",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wa', 'mouse', 'mouse', 'put', 'putting', 'eats', 'taken', 'took', 'connects', 'connecting,', 'I', 'saw', 'eighteen', 'mouse', 'today', 'and', 'today', 'and', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization with SPACY"
      ],
      "metadata": {
        "id": "4BzvkazOfIan"
      },
      "id": "4BzvkazOfIan"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sentence to be lemmatized\n",
        "sentence = \"was mice mouse put putting eats taken took connects connecting, I saw eighteen mice today and todays and yesterday\"\n",
        "\n",
        "# Parse the sentence with SpaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma_:{9}} {token.lemma:<{22}}')\n",
        "\n",
        "show_lemmas(doc)\n",
        "\n",
        "# Perform lemmatization on each token in the sentence\n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "print(\"\\n\",lemmatized_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyzBA2t7exDU",
        "outputId": "7cdc3906-3777-488a-904a-6ec314efa772"
      },
      "id": "HyzBA2t7exDU",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "was          AUX    be        10382539506755952630  \n",
            "mice         PROPN  mice      5736009664026643314   \n",
            "mouse        PROPN  mouse     1384165645700560590   \n",
            "put          VERB   put       11532405253426108718  \n",
            "putting      NOUN   putting   13773323934001429611  \n",
            "eats         NOUN   eat       9837207709914848172   \n",
            "taken        VERB   take      6789454535283781228   \n",
            "took         VERB   take      6789454535283781228   \n",
            "connects     NOUN   connect   8902972205924842345   \n",
            "connecting   VERB   connect   8902972205924842345   \n",
            ",            PUNCT  ,         2593208677638477497   \n",
            "I            PRON   I         4690420944186131903   \n",
            "saw          VERB   see       11925638236994514241  \n",
            "eighteen     NUM    eighteen  9609336664675087640   \n",
            "mice         NOUN   mouse     1384165645700560590   \n",
            "today        NOUN   today     11042482332948150395  \n",
            "and          CCONJ  and       2283656566040971221   \n",
            "todays       NOUN   today     11042482332948150395  \n",
            "and          CCONJ  and       2283656566040971221   \n",
            "yesterday    NOUN   yesterday 1756787072497230782   \n",
            "\n",
            " be mice mouse put putting eat take take connect connect , I see eighteen mouse today and today and yesterday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization with Sklearn"
      ],
      "metadata": {
        "id": "yJEgDOgPsXSG"
      },
      "id": "yJEgDOgPsXSG"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # Convert the text data into a matrix of token counts.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a WordNetLemmatizer instance\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a custom tokenizer function that performs lemmatization\n",
        "def tokenizer(text):\n",
        "    tokens = text.split()\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmas\n",
        "\n",
        "# Text data\n",
        "text_data = ['was mice mouse put putting eats',' taken took connects connecting',' I saw eighteen mice today and todays and yesterday']\n",
        "\n",
        "# Create a CountVectorizer instance with the custom tokenizer function\n",
        "vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
        "\n",
        "# Fit and transform the text data\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Get the vocabulary and feature names\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the lemmatized tokens and their corresponding feature names\n",
        "for token, feature_name in zip(vocabulary.keys(), feature_names):\n",
        "    print(f\"Lemmatized token: {token} --> Feature name: {feature_name}\")\n"
      ],
      "metadata": {
        "id": "ODT_-qAEexdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc4a21a-e806-426d-e186-8fe17459ad36"
      },
      "id": "ODT_-qAEexdH",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized token: wa --> Feature name: and\n",
            "Lemmatized token: mouse --> Feature name: connecting\n",
            "Lemmatized token: put --> Feature name: connects\n",
            "Lemmatized token: putting --> Feature name: eats\n",
            "Lemmatized token: eats --> Feature name: eighteen\n",
            "Lemmatized token: taken --> Feature name: i\n",
            "Lemmatized token: took --> Feature name: mouse\n",
            "Lemmatized token: connects --> Feature name: put\n",
            "Lemmatized token: connecting --> Feature name: putting\n",
            "Lemmatized token: i --> Feature name: saw\n",
            "Lemmatized token: saw --> Feature name: taken\n",
            "Lemmatized token: eighteen --> Feature name: today\n",
            "Lemmatized token: today --> Feature name: took\n",
            "Lemmatized token: and --> Feature name: wa\n",
            "Lemmatized token: yesterday --> Feature name: yesterday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization with keras"
      ],
      "metadata": {
        "id": "PTV7hFqDsgdI"
      },
      "id": "PTV7hFqDsgdI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement this in using keras or in DL\n",
        " we use spacy to convet the words into layers and pass them to layers in DL\n",
        "\n",
        "\n",
        "```\n",
        "import spacy\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"I have been running in the park.\"\n",
        "doc = nlp(text)\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "preprocessed_text = \" \".join(lemmas)\n",
        "\n",
        "# Define your Keras model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128))\n",
        "model.add(LSTM(units=64))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Tokenize and pad the preprocessed text\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts([preprocessed_text])\n",
        "sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=100)\n",
        "\n",
        "# Now you can use 'padded_sequence' as input to your Keras model for further processing\n",
        "model.predict(padded_sequence)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JBDGzHziqrx8"
      },
      "id": "JBDGzHziqrx8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6v49RyEsATr"
      },
      "id": "x6v49RyEsATr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop words\n",
        "\n",
        "Words that are frequently used in a language but are often considered of little value in text analysis"
      ],
      "metadata": {
        "id": "YvmUme-atnpu"
      },
      "id": "YvmUme-atnpu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of stop words in English include \"a\", \"an\", \"the\", \"is\", \"and\", \"in\", \"to\", \"of\", and \"it\".\n",
        "\n",
        "Stop words are removed from text data in order to reduce the noise, after rememonig them then the data is more likely to carry important semantic or contextual information."
      ],
      "metadata": {
        "id": "VsTzus0tt8HH"
      },
      "id": "VsTzus0tt8HH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing stopWords using NLTK"
      ],
      "metadata": {
        "id": "gYaQdE0zug25"
      },
      "id": "gYaQdE0zug25"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load the stop words\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Input text\n",
        "text = \"I love to read books and play sports.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Join the filtered tokens back into a sentence\n",
        "filtered_text = \" \".join(filtered_tokens)\n",
        "\n",
        "print(\"Filtered Text:\", filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEngdVQbvmO5",
        "outputId": "e163e545-e964-46f4-f302-c87e67d396c5"
      },
      "id": "sEngdVQbvmO5",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Text: love read books play sports .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing stopWords using SPACY"
      ],
      "metadata": {
        "id": "fQpOGZUNugw8"
      },
      "id": "fQpOGZUNugw8"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Parse the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "# Join the filtered tokens back into a sentence\n",
        "filtered_text = \" \".join(filtered_tokens)\n",
        "\n",
        "print(\"Filtered Text:\", filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxxX_23YwLlS",
        "outputId": "c67c3e4e-9c5d-421f-b46e-a6a535e75801"
      },
      "id": "hxxX_23YwLlS",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Text: love read books play sports .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing stopWords using Sklearn"
      ],
      "metadata": {
        "id": "CO_LKxSNugrf"
      },
      "id": "CO_LKxSNugrf"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Input text\n",
        "text = \"I love to read books and play sports.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = text.lower().split()\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
        "\n",
        "# Join the filtered tokens back into a sentence\n",
        "filtered_text = \" \".join(filtered_tokens)\n",
        "\n",
        "print(\"Filtered Text:\", filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHf7OKcJvD5-",
        "outputId": "a08393c0-739a-4b40-8a01-229891ce9604"
      },
      "id": "pHf7OKcJvD5-",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Text: love read books play sports.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "9b7d6863",
        "gOo5bmtmTnlF",
        "Spi60TJuZAS4",
        "QPBYuCPIhiwf",
        "4BzvkazOfIan",
        "yJEgDOgPsXSG",
        "PTV7hFqDsgdI"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}